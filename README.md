# Machine learning lectures

## Each folder represent my personal growth in the field of Machine Learning and Data Science

### Lecture 1 - Introduction
### Lecture 2 - Regression and Polynomial Regression
### Lecture 3 - KNN and Decision Trees
### Lecture 4 - Random Forest, ADAboost and XGBoost, Ensembling, Tune
### Elearning - Data support vectors



## Decision Trees used to classify the data in a graph 

<img src="https://user-images.githubusercontent.com/114572512/224649987-81984382-c5fc-49dd-86b6-a3704424784a.png" height="200" />



```text
    * Purity (0-1): the lower the better, degree to which a cluster or group contains only a single class or label,
    * Gini impurity index:  measure of the probability of incorrectly classifying a randomly
      chosen element in a dataset if it were randomly labeled according to the distribution of labels in the dataset.
    * Entropy: Entropy is a measure of the degree of disorder or randomness in a system, often associated with
      the amount of information or uncertainty present.
```
<img src="https://user-images.githubusercontent.com/114572512/224651997-08d8a795-3e82-4039-aea6-c7d0ef25d618.png" width="400" />

```text

Advanatages: 
   Very simple works iwth non linear problems 
   interpretable 
   computationally cheap

Problems:
  Unstable ( a change in the data can lead to completely differtent model)
  mostly baed in heristic with no solid  stattisical background
  prone to overfit
  
  
 If i have to classify smth on a graph  that looks like clustering = decision tree

```

## Random Forest

## ADAboost

## XGBoost

## Parameter Tuning
   
## Suppor Vector Machine 
